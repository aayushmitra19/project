{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3gkt4KTJlUK"
   },
   "source": [
    "#**Tweet Sentiment Analysis using RNN and LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hpWuZ5VAera"
   },
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.0-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "     ---------------------------------------- 12.3/12.3 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.8/96.8 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.11-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp39-cp39-win_amd64.whl (482 kB)\n",
      "     -------------------------------------- 482.8/482.8 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: cymem, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, colorama, catalogue, blis, wasabi, srsly, preshed, typer, confection, thinc, pathy, spacy\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.5\n",
      "    Uninstalling colorama-0.4.5:\n",
      "      Successfully uninstalled colorama-0.4.5\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 colorama-0.4.6 confection-0.1.0 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.11 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aayush\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8GvOE8TRXYE6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from keras.layers import Dropout\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 6735,
     "status": "ok",
     "timestamp": 1687075953379,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "QnW-tJ0eaOIn",
    "outputId": "0dce8bea-fdbd-4e3b-cd67-bcfafc35022f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101154</th>\n",
       "      <td>4</td>\n",
       "      <td>@jessikasay congrats on the followers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101155</th>\n",
       "      <td>4</td>\n",
       "      <td>@dannywood As in &amp;quot;back dat ass on up in h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101156</th>\n",
       "      <td>4</td>\n",
       "      <td>School In about 12 minutes then I get picked u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101157</th>\n",
       "      <td>4</td>\n",
       "      <td>@itsMeeeech -- sounds fun!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101158</th>\n",
       "      <td>4</td>\n",
       "      <td>Good morning everyone  Welcome to my new follo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101159 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target                                              tweet\n",
       "0            0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1            0  is upset that he can't update his Facebook by ...\n",
       "2            0  @Kenichan I dived many times for the ball. Man...\n",
       "3            0    my whole body feels itchy and like its on fire \n",
       "4            0  @nationwideclass no, it's not behaving at all....\n",
       "...        ...                                                ...\n",
       "101154       4             @jessikasay congrats on the followers \n",
       "101155       4  @dannywood As in &quot;back dat ass on up in h...\n",
       "101156       4  School In about 12 minutes then I get picked u...\n",
       "101157       4                        @itsMeeeech -- sounds fun! \n",
       "101158       4  Good morning everyone  Welcome to my new follo...\n",
       "\n",
       "[101159 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('TweetSentiment_60 (1) (1).xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YS1rHiqmZFbm"
   },
   "source": [
    "#Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "761IBcajG-P0"
   },
   "outputs": [],
   "source": [
    "df[\"tweet\"] = df[\"tweet\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE-XoBW0Hvpl"
   },
   "source": [
    "###**Lower Casing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1687075953381,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "yZIAM4GwHp8e",
    "outputId": "39cdc3d1-863d-435c-a6f5-9f68de4003e9"
   },
   "outputs": [],
   "source": [
    "df[\"processed_tweets\"] = df[\"tweet\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RlHCBgxH2qS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1687075953382,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "JLOM-a0iJ0rC",
    "outputId": "e30dc161-765e-4bb8-d707-b8a21307665b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nOriginal Tweet:\",df['tweet'][0])\n",
    "print(df['processed_tweets'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cil-nLteZ9o3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1687075953384,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "YwgfI3eqba5Y",
    "outputId": "644c1191-ebc2-4e03-f4b9-9623fb783e57"
   },
   "outputs": [],
   "source": [
    "len(df['processed_tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkA3j05TIAim"
   },
   "source": [
    "###**Removal of @ and User names**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWQmnri2sejY"
   },
   "source": [
    "<!-- Tokenization -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38733,
     "status": "ok",
     "timestamp": 1687075992099,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "RZIR1xmoNLPf",
    "outputId": "afe2bd79-b605-404c-9e54-d4b1f12a8920"
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "for a in range(len(df['processed_tweets'])):\n",
    "\n",
    "  result = tknzr.tokenize(df['processed_tweets'][a])\n",
    "  res=\" \".join(result)\n",
    "  df['processed_tweets'][a]=res\n",
    "print(\"\\nTokenize a twitter text:\")\n",
    "print(df['processed_tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1687075992101,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "XPFoJsK0ONJV",
    "outputId": "2b0c91aa-19d5-44ce-d17c-f5aa2862fa5e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-lgn2bDtFcw"
   },
   "source": [
    "###**Removing Punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1687075992103,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "VyjXaoXGX-XX",
    "outputId": "b50a4a7e-7a29-4312-aaf8-e401c8d36b61"
   },
   "outputs": [],
   "source": [
    "# dropping the new column created in last cell\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df[\"processed_tweets\"] = df[\"processed_tweets\"].apply(lambda text: remove_punctuation(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dol3gKZWeXwK"
   },
   "source": [
    "###**Removing URLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe8MBpG2ddEK"
   },
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "# def remove_urls(text):\n",
    "#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "#     return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31722,
     "status": "ok",
     "timestamp": 1687076023804,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "evmAyptXeaVw",
    "outputId": "079d61bd-b53d-4c44-ce76-b9cb6428ba71"
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "for a in range(len(df['processed_tweets'])):\n",
    "\n",
    "  result = remove_URL(df['processed_tweets'][a])\n",
    "  df['processed_tweets'][a]=result\n",
    "print(\"No URLs:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1687076023804,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "1JJUenJAe_N3",
    "outputId": "322e279c-d1d2-4ffd-9adb-23145846110b"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv1bt-U4jOLA"
   },
   "source": [
    "###**Removing Emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1687076023804,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "UqHv0WPPf1Wt",
    "outputId": "b9e9ec83-ce95-48aa-81c9-ebbb9aafd9cf"
   },
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "remove_emoji(\"game is on 🔥🔥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27083,
     "status": "ok",
     "timestamp": 1687076051641,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "hOx6FEmViNm2",
    "outputId": "1eb18071-51a2-44b4-8357-1330eb8b422e"
   },
   "outputs": [],
   "source": [
    "for a in range(len(df['processed_tweets'])):\n",
    "\n",
    "  result = remove_emoji(df['processed_tweets'][a])\n",
    "  df['processed_tweets'][a]=result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YEDyZ8yjWCT"
   },
   "source": [
    "###**Removal of Emoticons¶**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hG9AwKh_iWQA"
   },
   "outputs": [],
   "source": [
    "# Thanks : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\n",
    "EMOTICONS = {\n",
    "    u\":‑\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‑\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‑\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‑':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‑O\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‑o\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‑0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‑\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‑\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‑,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‑\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‑\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‑\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‑\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‑\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‑\\)\":\"Cool\",\n",
    "    u\"\\|‑O\":\"Bored\",\n",
    "    u\":‑J\":\"Tongue-in-cheek\",\n",
    "    u\"#‑\\)\":\"Party all night\",\n",
    "    u\"%‑\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‑\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(一一\\)\":\"Shame\",\n",
    "    u\"\\(；一_一\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\・\\・?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\（\\^—\\^\\）\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(ーー;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1687076051644,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "molcH8i5jkf7",
    "outputId": "86da4c53-c9f8-4a90-c86f-72b909186da2"
   },
   "outputs": [],
   "source": [
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoticons(\"Hello :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40096,
     "status": "ok",
     "timestamp": 1687076091691,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "WiktfqM7jo51",
    "outputId": "0c08ce40-2cdc-4175-995d-59803a67e037"
   },
   "outputs": [],
   "source": [
    "for a in range(len(df['processed_tweets'])):\n",
    "\n",
    "  result = remove_emoticons(df['processed_tweets'][a])\n",
    "  df['processed_tweets'][a]=result\n",
    "print(\"No Emoticons:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1687076091693,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "aRkylyRvkXZr",
    "outputId": "bae3ef38-15e8-46fe-cde1-99a7cd7ee185"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1687076091694,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "unXrFpbpkyFN",
    "outputId": "4237557e-7732-4703-fb09-a3632a7534f0"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBS-rS14lVGo"
   },
   "source": [
    "### **Labelling the target fileds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1687076091695,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "rEteaEmDk7nq",
    "outputId": "92e2fabf-ae91-48dd-d0ec-aceeb650d5b7"
   },
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiXm8TK2lr8S"
   },
   "source": [
    "0 --> negative,\n",
    "\n",
    "4 --> Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMKYNLa3A8rC"
   },
   "source": [
    " Encoding negative and positive as 0 and 1 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1687076091696,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "mR8RpXFZlkGJ",
    "outputId": "4a310854-5376-457d-e894-1b0da71abef1"
   },
   "outputs": [],
   "source": [
    "df['Target']=df['target'].replace(4,1)\n",
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLEdpoqAl8SB"
   },
   "source": [
    "0 --> negative,\n",
    "\n",
    "1 --> Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOaI62jlygrL"
   },
   "source": [
    "[Reference for maxlen](https://stackoverflow.com/questions/42943291/what-does-keras-io-preprocessing-sequence-pad-sequences-do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXgoeWUV66Jm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# np.random.seed(1)\n",
    "# tf.set_random_seed(2)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "# from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ngVo1Njym2q"
   },
   "source": [
    "###**Stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHYkMlsIyQxL"
   },
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1687076293802,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "uDsYr2jOyTZY",
    "outputId": "dc28be3d-6bef-4194-892f-552b6f3e5962"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "\n",
    " #apply function\n",
    "df['processed_tweets'] = df['processed_tweets'].apply(lambda text: cleaning_stopwords(text))\n",
    "df['processed_tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1687076294429,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "4EWzYdIsxcN2",
    "outputId": "311d80cc-29fc-4a3e-f0b6-579743dbffc1"
   },
   "outputs": [],
   "source": [
    "dataset = df.drop('tweet',axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKpHJhUE_6TM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izFN_HndBeRZ"
   },
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14089,
     "status": "ok",
     "timestamp": 1687076314904,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "xTXfD-qn_xG2",
    "outputId": "b6e81ec4-390e-442b-9b2a-2cee0e88e506"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df['processed_tweets'] = df['processed_tweets'].apply(lambda x: lemmatizer_on_text(x))\n",
    "df['processed_tweets'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3il0-Co-6zv"
   },
   "outputs": [],
   "source": [
    "X = df['processed_tweets']\n",
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1687076344041,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "Vfsu88fu6tWT",
    "outputId": "f424da58-6508-49ca-e10f-b714bc7bd69d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(X, y,random_state = 42, test_size=0.1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1687076346210,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "-SNfXhSuArZe",
    "outputId": "9c41e465-8c94-4c99-a2c1-159d9d6ac760"
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1687076348547,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "sBjjNPm2AzEc",
    "outputId": "55c2f11a-1bd3-45e6-b5b6-ab686289702b"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGju4DdaoVlz"
   },
   "source": [
    "##**Text Sequencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4128,
     "status": "ok",
     "timestamp": 1687076356773,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "etWEVeHGyTPl",
    "outputId": "6e9a13f8-d7f9-4f48-adcb-0f32ca15d2b6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# tokenize and sequence sentences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "train_text_vec = tokenizer.texts_to_sequences(X_train)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "test_text_vec = tokenizer.texts_to_sequences(X_test)\n",
    "test_text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHQNpcvC3OYq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggUYjSWPy_rt"
   },
   "outputs": [],
   "source": [
    "# print('Maximum tweet length: {}'.format(\n",
    "# len((max((train_text_vec + test_text_vec), key=len)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1687076364932,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "Mvvw_FKoLZgM",
    "outputId": "1904a126-65ee-46fc-dce0-d4fee161865d"
   },
   "outputs": [],
   "source": [
    "#checking the next maximum number\n",
    "lenght = []\n",
    "for i in (train_text_vec + test_text_vec):\n",
    "  if  10<len(i)<28:\n",
    "    lenght.append(len(i))\n",
    "max(lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sPjEZemt-zP"
   },
   "source": [
    "We can leave the maximum word length to 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZsZoyDCM3DO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyvgddeWMINq"
   },
   "outputs": [],
   "source": [
    "## train['processed_tweets'].apply(lambda x : len(x.split(' '))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1687076523087,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "TsMEdDf9y_je",
    "outputId": "fa94aae8-a19b-4094-a3bf-ddcc3fcb6eb4"
   },
   "outputs": [],
   "source": [
    "# pad the sequences\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_words = 28\n",
    "X_train = sequence.pad_sequences(train_text_vec, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(test_text_vec, maxlen=max_words)\n",
    "# modelCNN = None\n",
    "modelRNN=None\n",
    "modelLSTM=None\n",
    "print (\"After Padding X[train[0]=\\n\", X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fGDTDeTN5WL"
   },
   "source": [
    "##**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1687076539947,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "EeWISUU0YbNX",
    "outputId": "450fe547-1c2a-4924-e341-ef2d97c7eadf"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1687076541537,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "TE1m88wDOLPs",
    "outputId": "7c4dc7ca-d039-42f4-f583-1bb66ef19732"
   },
   "outputs": [],
   "source": [
    "len((train_text_vec + test_text_vec))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fkyoj9L4yLJm"
   },
   "outputs": [],
   "source": [
    "\n",
    "vocabulary_size = len((train_text_vec + test_text_vec))*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpS81fvt5XUV"
   },
   "source": [
    "#Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10984,
     "status": "ok",
     "timestamp": 1687076667022,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "RDoWTQWQyLEE",
    "outputId": "eac029cb-396e-4beb-aeeb-3da9318dfe46"
   },
   "outputs": [],
   "source": [
    "#Glove\n",
    "embeddings_index = {}\n",
    "with open('/content/drive/MyDrive/Data/glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I55JVJt7D_Yt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-79anUm2yLBc"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((vocab_size,300))\n",
    "# for word,i in tqdm(token.word_index.items()):\n",
    "#     embedding_value = embedding_vector.get(word)\n",
    "#     if embedding_value is not None:\n",
    "#         embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVJKPRqUyK-4"
   },
   "outputs": [],
   "source": [
    "#embedding matrix\n",
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii_ifjMX96eb"
   },
   "source": [
    "#Recurrent Neural Network(RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avocMKZFu_HM"
   },
   "outputs": [],
   "source": [
    "#Now create a simple RNN model and lets see the accuracy\n",
    "from keras.layers import SimpleRNN\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1687076684654,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "N1Vcm0TdyK4R",
    "outputId": "8bb5f1fa-b829-470e-8d77-7f899e357d55"
   },
   "outputs": [],
   "source": [
    "embedding_size=100\n",
    "\n",
    "\n",
    "modelRNN=Sequential()\n",
    "modelRNN.add(Embedding(vocabulary_size,embedding_size,weights = [embedding_matrix],input_length=max_words)) #embdsize\n",
    "# modelRNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelRNN.add(Dropout(0.70))\n",
    "modelRNN.add(SimpleRNN(150,activation = \"tanh\",kernel_regularizer=regularizers.l2(0.01)))\n",
    "modelRNN.add(Dropout(0.15))\n",
    "modelRNN.add(Dense(1, activation='sigmoid'))\n",
    "print(modelRNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdQbFFvOyKyy"
   },
   "outputs": [],
   "source": [
    "modelRNN.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHdqFP5yyKvj"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 20\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "callback_listRNN  = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_modRNN_BestValAcc.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204623,
     "status": "ok",
     "timestamp": 1687076896344,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "pJRJQ4liyKsc",
    "outputId": "5d5124f3-77ff-4c26-a0a2-2ee3e636d43b"
   },
   "outputs": [],
   "source": [
    "history = modelRNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs,\n",
    "             callbacks=callback_listRNN)\n",
    "modelRNN.save_weights(\"my_modRNN_Latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1687076897595,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "1uDZ-eszqxsA",
    "outputId": "d194e20b-b813-4a30-e949-4968d5e2774e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy-RNN')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss-RNN')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yq1KecvA52u"
   },
   "source": [
    "#Loading the last epoch modeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVcbhMBwyKg-"
   },
   "outputs": [],
   "source": [
    "#Loading the last epoch model\n",
    "modelRNN=Sequential()\n",
    "modelRNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelRNN.add(SimpleRNN(150,activation = \"tanh\"))\n",
    "modelRNN.add(Dense(1, activation='sigmoid'))\n",
    "print(modelRNN.summary())\n",
    "modelRNN.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "modelRNN.load_weights(\"my_modRNN_Latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMV8YmS1_56J"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpFUfluR_1xk"
   },
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4Nl8q4M836E"
   },
   "outputs": [],
   "source": [
    "callback_listLSTM = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_modLSTM_BestValAcc.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhKvwUDDKVaJ"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1687076921751,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "0fmxmT04KVc1",
    "outputId": "0064239c-ebb2-4f0f-ca38-8675fd660466"
   },
   "outputs": [],
   "source": [
    "embedding_size=100\n",
    "modelLSTM=Sequential()\n",
    "modelLSTM.add(Embedding(vocabulary_size,100,weights = [embedding_matrix],input_length=28)) #embdsize\n",
    "# modelRNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelLSTM.add(Dropout(0.70))\n",
    "modelLSTM.add(LSTM(150,activation = \"tanh\",kernel_regularizer=regularizers.l2(0.01)))\n",
    "modelLSTM.add(Dropout(0.15))\n",
    "modelLSTM.add(Dense(1, activation='sigmoid'))\n",
    "print(modelLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXUfX40tnrmm"
   },
   "outputs": [],
   "source": [
    "modelLSTM.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VbJutfKKVgV"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 20\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "callback_listLSTM = [#early1\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_modRNN_BestValAcc.h5\", monitor=\"val_acc\",\n",
    "                                               save_best_only=True),\n",
    "                #keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143571,
     "status": "ok",
     "timestamp": 1687077075175,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "j0Qr9BI7n-gJ",
    "outputId": "d460b72a-788b-4d42-ed81-8a6e16def68f"
   },
   "outputs": [],
   "source": [
    "LSTMhistory = modelLSTM.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs,\n",
    "             callbacks=callback_listLSTM)\n",
    "modelLSTM.save_weights(\"my_modLSTM_Latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmDts0y9y-mk"
   },
   "source": [
    "##**Performance Measure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "executionInfo": {
     "elapsed": 1497,
     "status": "ok",
     "timestamp": 1687077086652,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "sv270PifPAY2",
    "outputId": "5777131f-31b8-4cad-b87f-0b96f20c825b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = modelLSTM.history.history['accuracy']\n",
    "val_acc = modelLSTM.history.history['val_accuracy']\n",
    "loss = modelLSTM.history.history['loss']\n",
    "val_loss = modelLSTM.history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy-LSTM')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss-LSTM')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTgajSeWSaKX"
   },
   "source": [
    "###**Confusion matrix and classification_report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMiNPDx0KVmv"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3268,
     "status": "ok",
     "timestamp": 1687077120280,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "4vDCYDrrFWvd",
    "outputId": "49887655-3b6b-401e-817c-70af344c21bb"
   },
   "outputs": [],
   "source": [
    "y_predict = modelRNN.predict(X_test)\n",
    "print(classification_report(y_test, y_predict.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWVZBq58MgBq"
   },
   "outputs": [],
   "source": [
    "cf_matrix=confusion_matrix(y_test, y_predict.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 885,
     "status": "ok",
     "timestamp": 1687077141353,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "qx10tjNgMmw3",
    "outputId": "19a2c3a9-23ba-4720-adea-bbb641059427"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg1AwzT2Spqy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adLT8g2-1MZh"
   },
   "source": [
    "##**Custom Tweet For Extra Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y79arbQCNomW"
   },
   "outputs": [],
   "source": [
    "test_data=['today is a sad day','he was happy today','i do not like you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1687077339586,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "wX7c3vx1MtBg",
    "outputId": "90df6ef9-b751-4eee-d5bd-69e8474d2e24"
   },
   "outputs": [],
   "source": [
    "x_test  = np.array( tokenizer.texts_to_sequences(test_data))\n",
    "x_test = pad_sequences(x_test, maxlen=28)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1687077358742,
     "user": {
      "displayName": "Ashesh Xalxo",
      "userId": "17490657637613382474"
     },
     "user_tz": -330
    },
    "id": "TQMNIjy7OTgC",
    "outputId": "416be7bb-23ec-4dc1-a2b6-e61dd78a7f00"
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(test_data)):\n",
    "  print('Test sentence:-',test_data[i])\n",
    "\n",
    "  res=modelRNN.predict(x_test)\n",
    "  if res[i]>0.3:\n",
    "    print('Sentiment type:Positive',res[i])\n",
    "  else:\n",
    "    print('Sentiment type:Negative',res[i])\n",
    "  print('==================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_7SA6XusRQf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "17re2iPAJYY8AKwuSpRwlxyDAOwUJuYm2",
     "timestamp": 1687083774784
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
